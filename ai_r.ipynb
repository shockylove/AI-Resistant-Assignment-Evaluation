{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ad5192",
   "metadata": {},
   "source": [
    "# AI-Resistant Assignment Evaluation & Redesign Demo\n",
    "\n",
    "This notebook runs:\n",
    "1. **Simulate** an AI “student” answer  \n",
    "2. **Assess** the assignment vulnerability + grade the answer  \n",
    "3. **Redesign** suggestions to make it AI-resistant  \n",
    "4. **Report** assembly  \n",
    "\n",
    "Make sure you have:\n",
    "- A `.env` file in this folder with `OPENAI_API_KEY=sk-...`\n",
    "- `assignment.pdf` or `.txt` in `assignment_examples/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14a21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# Uncomment if you need to install dependencies in your notebook env\n",
    "# pip install openai langchain langchain-community pyyaml python-dotenv PyPDF2\n",
    "\n",
    "# %%python\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Utility: display Markdown nicely in Jupyter\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754ccd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Assignment Prompt:**\n",
       "\n",
       "HW2: Code Generation and Prompting\n",
       "CS598LMZ Spring 2025\n",
       "1 Goal\n",
       "In this assignment you will use code models for program synthesis. The goal is to test how code\n",
       "LLMs can solve programming problems.\n",
       "1.1 HumanEval benchmark\n",
       "def common (l1: list, l2: list): \n",
       "    \"\"\"Return sorted unique common elements for two lists\"\"\" \n",
       "[5,3,2,8], [3,2] \n",
       "[4,3,2,8], [3,2,4] [4,3,2,8], [] H UMAN E V AL input \n",
       "    common_elements = list (set(l1). intersection (set(l2))) \n",
       "    common_elements. sort ()\n",
       "    return common_ele..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your API key\n",
    "load_dotenv()  # expects a .env file in this folder\n",
    "\n",
    "# Helper to load PDF or TXT\n",
    "def load_assignment(path: str) -> str:\n",
    "    if path.lower().endswith(\".pdf\"):\n",
    "        reader = PdfReader(path)\n",
    "        return \"\\n\\n\".join(p.extract_text() or \"\" for p in reader.pages)\n",
    "    else:\n",
    "        return open(path, \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# Example prompt path\n",
    "ASSIGNMENT_PATH = \"assignment_examples/CS598LMZ_HW2_sp25.pdf\"\n",
    "assignment_text = load_assignment(ASSIGNMENT_PATH)\n",
    "display(Markdown(\"**Assignment Prompt:**\\n\\n\" + assignment_text[:500] + \"...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY or API_KEY == \"your_key_here\":\n",
    "    # ask you to type it in\n",
    "    API_KEY = input(\"Enter your OpenAI API key: \").strip()\n",
    "\n",
    "# later, when you init the LLM:\n",
    "llm = ChatOpenAI(model=\"o4-mini\", openai_api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cfb1dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## AI-Generated Answer\n",
       "\n",
       "Below is the report.md that summarizes our HW2 runs, followed by the code snippets and discussion of base vs. improved synthesis. All references to course materials are included at the end.\n",
       "\n",
       "------------------------------------------------------------\n",
       "report.md\n",
       "------------------------------------------------------------\n",
       "\n",
       "1) Results of both runs on HumanEval (greedy pass@1) using EvalPlus:\n",
       "\n",
       "Base Run:  \n",
       " • Base score           = 19.1%  \n",
       " • Base + Extra score   = 20.4%  \n",
       "\n",
       "Improved Run:  \n",
       " • Base s..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simulate (AI Student)\n",
    "# %%python\n",
    "def simulate_assignment(text: str) -> str:\n",
    "    llm = ChatOpenAI(model=\"o4-mini\", openai_api_key=API_KEY,temperature=1.0)\n",
    "    prompt = (\n",
    "        \"You are a third-year university student with two hours to complete the assignment. \"\n",
    "        \"Write an answer of at least 800 words, citing relevant course materials.\\n\\n\"\n",
    "        f\"{text}\\n\\nBegin your answer now:\"\n",
    "    )\n",
    "    resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return resp.content\n",
    "\n",
    "ai_answer = simulate_assignment(assignment_text)\n",
    "# save or display\n",
    "with open(\"ai_answer.md\",\"w\",encoding=\"utf-8\") as f: f.write(ai_answer)\n",
    "display(Markdown(\"## AI-Generated Answer\\n\\n\" + ai_answer[:500] + \"...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a416eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def safe_parse_json(raw: str) -> dict:\n",
    "    \"\"\"\n",
    "    Strip anything before/after the first {…} and load JSON.\n",
    "    \"\"\"\n",
    "    raw = raw.strip()\n",
    "    if not raw.startswith(\"{\"):\n",
    "        m = re.search(r\"(\\{.*\\})\", raw, re.DOTALL)\n",
    "        if m:\n",
    "            raw = m.group(1)\n",
    "    return json.loads(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77efd749",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'safe_parse_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     95\u001b[39m     raw = llm.invoke([HumanMessage(content=prompt)]).content\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_parse_json(raw)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m assignment_analysis = \u001b[43manalyze_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43massignment_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m answer_assessment   = grade_answer(assignment_text, ai_answer)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Package up the results exactly as our pipeline expect\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36manalyze_assignment\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     40\u001b[39m prompt = (\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou are an educational assessment expert. Analyze the following assignment prompt \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m**only** according to these five dimensions, and **respond with a single valid JSON object and nothing else**:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m )\n\u001b[32m     56\u001b[39m raw = llm.invoke([HumanMessage(content=prompt)]).content\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafe_parse_json\u001b[49m(raw)\n",
      "\u001b[31mNameError\u001b[39m: name 'safe_parse_json' is not defined"
     ]
    }
   ],
   "source": [
    "# Assess\n",
    "# Load rubric\n",
    "with open(\"rubric.yaml\",\"r\",encoding=\"utf-8\") as f:\n",
    "    RUBRIC = yaml.safe_load(f)\n",
    "\n",
    "def analyze_assignment(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the assignment for AI‐vulnerability across five dimensions.\n",
    "    Returns a dict:\n",
    "      { dimension: {score: 0–100, rationale: \"…\" }, … }\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"retrievability\": (\n",
    "            \"Answer can be directly retrieved from publicly available sources \"\n",
    "            \"without context constraints.\"\n",
    "        ),\n",
    "        \"templatedness\": (\n",
    "            \"Response follows a highly generic formulaic structure lacking \"\n",
    "            \"specific context (e.g., a five‐paragraph essay with no proper names).\"\n",
    "        ),\n",
    "        \"context_dependence\": (\n",
    "            \"Does not require integration of real‐world context or personal experience.\"\n",
    "        ),\n",
    "        \"counterfactual_need\": (\n",
    "            \"Does not demand counterfactual reasoning, evaluation, or hypothesis \"\n",
    "            \"(e.g., no prompts asking 'explain why not').\"\n",
    "        ),\n",
    "        \"process_orientation\": (\n",
    "            \"Does not require submission of intermediate work (drafts, outlines), only a final answer.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"o4-mini\",\n",
    "        openai_api_key=API_KEY,\n",
    "        # o4-mini only supports temperature=1.0 by default\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an educational assessment expert. Analyze the following assignment prompt \"\n",
    "        \"**only** according to these five dimensions, and **respond with a single valid JSON object and nothing else**:\\n\\n\"\n",
    "        \"For each dimension, assign a risk score from 0 to 100 (higher means more vulnerable), and provide a brief rationale.\"\n",
    "        f\"{yaml.dump(metrics, allow_unicode=True)}\\n\\n\"\n",
    "        f\"Assignment:\\n{text}\\n\\n\"\n",
    "        \"The JSON must be of the form:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"retrievability\": {\"score\": <0-100>, \"rationale\": \"…\"},\\n'\n",
    "        '  \"templatedness\": {\"score\": <0-100>, \"rationale\": \"…\"},\\n'\n",
    "        '  \"context_dependence\":{\"score\": <0-100>, \"rationale\": \"…\"},\\n'\n",
    "        '  \"counterfactual_need\": {\"score\": <0-100>, \"rationale\": \"…\"},\\n'\n",
    "        '  \"process_orientation\": {\"score\": <0-100>, \"rationale\": \"…\"},\\n'\n",
    "        \"}\\n\"\n",
    "    )\n",
    "\n",
    "    raw = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    return safe_parse_json(raw)\n",
    "\n",
    "\n",
    "\n",
    "def grade_answer(text: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Grade the given answer against the rubric.\n",
    "    Returns a dict:\n",
    "      {\n",
    "        total_score: 0–100,\n",
    "        breakdown: { criterion: score, … },\n",
    "        strengths: \"…\",\n",
    "        weaknesses: \"…\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    rubric_text = yaml.dump(RUBRIC, allow_unicode=True)\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"o4-mini\",\n",
    "        openai_api_key=API_KEY,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a strict teaching assistant. Grade the student’s answer according to the rubric below \"\n",
    "        \"and **respond with valid JSON only** (no extra text):\\n\\n\"\n",
    "        f\"{rubric_text}\\n\\n\"\n",
    "        f\"Assignment:\\n{text}\\n\\n\"\n",
    "        f\"Student Answer:\\n{answer}\\n\\n\"\n",
    "        \"The JSON schema must be:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"total_score\\\": <0-100>,\\n\"\n",
    "        \"  \\\"breakdown\\\": {\\\"criterion\\\": <0-criterion_weight>, …},\\n\"\n",
    "        \"  \\\"strengths\\\": \\\"…\\\",\\n\"\n",
    "        \"  \\\"weaknesses\\\": \\\"…\\\"\\n\"\n",
    "        \"}\\n\"\n",
    "    )\n",
    "\n",
    "    raw = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    return safe_parse_json(raw)\n",
    "\n",
    "\n",
    "assignment_analysis = analyze_assignment(assignment_text)\n",
    "answer_assessment   = grade_answer(assignment_text, ai_answer)\n",
    "\n",
    "\n",
    "# Package up the results exactly as our pipeline expect\n",
    "evaluation = {\n",
    "    \"assignment_analysis\": assignment_analysis,\n",
    "    \"answer_assessment\": answer_assessment\n",
    "}\n",
    "\n",
    "# Write to evaluation.json\n",
    "with open(\"evaluation.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ evaluation.json saved\")\n",
    "\n",
    "display(Markdown(\"### Assignment Analysis\\n\" + \n",
    "                 \"\\n\".join(f\"- **{k}**: {v}\" for k,v in assignment_analysis.items())))\n",
    "display(Markdown(\"### Answer Assessment\\n\" + json.dumps(answer_assessment, indent=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redesign Suggestions\n",
    "def redesign(assignment: str, analysis: dict, assess: dict) -> str:\n",
    "    llm = ChatOpenAI(model=\"o4-mini\", openai_api_key=API_KEY,temperature=1.0)\n",
    "    prompt = (\n",
    "        \"You are an instructional designer. Given:\\n\\n\"\n",
    "        f\"Assignment:\\n{assignment}\\n\\n\"\n",
    "        f\"Vulnerability Analysis:\\n{json.dumps(analysis,indent=2)}\\n\\n\"\n",
    "        f\"Weaknesses:\\n{assess['weaknesses']}\\n\\n\"\n",
    "        \"Propose 3 AI-resistant redesign suggestions with rationale and examples.\"\n",
    "    )\n",
    "    return llm.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "suggestions_md = redesign(assignment_text, assignment_analysis, answer_assessment)\n",
    "with open(\"suggestions.md\",\"w\",encoding=\"utf-8\") as f: f.write(suggestions_md)\n",
    "display(Markdown(\"## Redesign Suggestions\\n\\n\" + suggestions_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble Report\n",
    "\n",
    "# Build final report in a variable\n",
    "report_lines = []\n",
    "report_lines.append(\"# Final AI-Resistant Assignment Report\\n\")\n",
    "report_lines.append(\"## 1. Prompt\\n\" + assignment_text + \"\\n\")\n",
    "report_lines.append(\"## 2. AI Answer\\n\" + ai_answer + \"\\n\")\n",
    "report_lines.append(\"## 3. Vulnerability Analysis\\n\" + json.dumps(assignment_analysis, indent=2) + \"\\n\")\n",
    "report_lines.append(\"## 4. Assessment\\n\" + json.dumps(answer_assessment, indent=2) + \"\\n\")\n",
    "report_lines.append(\"## 5. Redesign Suggestions\\n\" + suggestions_md + \"\\n\")\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "with open(\"report.md\",\"w\",encoding=\"utf-8\") as f: f.write(final_report)\n",
    "display(Markdown(final_report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_resistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
